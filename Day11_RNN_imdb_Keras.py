# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ib9kTGVu0K-hreU1eagt6ZRyhzHXUKrE
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
from keras.datasets import imdb

# 영화 리뷰는 X_train에, 감성 정보는 y_train에 저장된다.
# 테스트용 리뷰는 X_test에, 테스트용 리뷰의 감성 정보는 y_test에 저장된다.
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 5000)
# 가장 빈도수가 높은 5000개의 단어만 사용

len(x_train), len(x_test)

len_result = [len(s) for s in x_train]

print('리뷰의 최대 길이 : {}'.format(np.max(len_result)))
print('리뷰의 평균 길이 : {}'.format(np.mean(len_result)))

plt.subplot(1,2,1)
plt.boxplot(len_result)
plt.subplot(1,2,2)
plt.hist(len_result, bins=50)
plt.show()

unique_elements, counts_elements = np.unique(y_train, return_counts = True)
np.asarray((unique_elements, counts_elements))
# True, False값이 절반씩

import re
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, GRU, Embedding, SimpleRNN
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model

x_train = pad_sequences(x_train, maxlen = 500)
x_test = pad_sequences(x_test, maxlen = 500)
# pad_sequence: 학습 모델에 넣으려면 모두 길이가 같아야함 그걸 맞춰줌. 500보다 짧으면 0, 길면 자르기

model = Sequential()
model.add(Embedding(5000, 100))
# Embedding: 단어를 밀집벡터로 만듬
# 첫번째 인자: 단어 집합의 크기, 두번째 인자: 임베딩 후의 벡터 크기
# model.add(GRU(128))
model.add(SimpleRNN(50, return_sequences = True))
model.add(SimpleRNN(50, return_sequences = True))
model.add(SimpleRNN(50))
model.add(Dense(1, activation = 'sigmoid'))

model.summary()

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
mc = ModelCheckpoint('GRU_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)
# 검증 데이터의 손실(loss)이 증가하면, 과적합 징후이므로 검증 데이터 손실이 4회 증가하면 학습을 중단하는 조기 종료(EarlyStopping)를 사용합니다.
# 또한, ModelCheckpoint를 사용하여 검증 데이터의 정확도가 이전보다 좋아질 경우에만 모델을 저장하도록 합니다.

from keras.optimizers import SGD, Adam

model.compile(optimizer = Adam(lr = 0.001), loss = 'binary_crossentropy', metrics = ['acc'])
history = model.fit(x_train, y_train, epochs = 20, callbacks = [es, mc], batch_size = 32, validation_split = 0.2)