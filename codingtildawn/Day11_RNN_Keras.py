# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XqLw-UP4aQOiXg_gc7rEblYR1C2rrC-M
"""

from keras.datasets import imdb

# 자주쓰이는 단어 5000개

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 5000)

idx_to_word = {v: k for k, v in imdb.get_word_index().items()}

' '.join([idx_to_word[idx] for idx in x_train[0]])

x_train.shape, y_train

import numpy as np
from tqdm import tqdm_notebook
# tqdm: 반복문에 대한 로딩바를 생성

n_input_words = 10

x_train_rnn = []
y_train_rnn = []
for sentence in tqdm_notebook(x_train):
    for i in range(len(sentence) - n_input_words):
        x = sentence[i:i+n_input_words]
        y = sentence[i+n_input_words]
        x_train_rnn.append(x)
        y_train_rnn.append(y)

x_train_rnn = np.array(x_train_rnn)
y_train_rnn = np.array(y_train_rnn)

x_train_rnn.shape, y_train_rnn.shape

# 0부터 5717841까지 섞은 1차배열 생성

idx = np.random.permutation(len(x_train_rnn))

x_train_rnn = x_train_rnn[idx[:40000]]
y_train_rnn = y_train_rnn[idx[:40000]]

x_train_rnn.shape, y_train_rnn.shape

from keras.utils import to_categorical

x_train_rnn = to_categorical(x_train_rnn,num_classes = 5000)
y_train_rnn = to_categorical(y_train_rnn,num_classes = 5000)

x_train_rnn.shape, y_train_rnn.shape

from keras.layers import *
from keras.models import Model, Sequential

def build_model():
    model = Sequential()

    #return_sequence = False: 마지막만 취함, True: 모두 취함
    model.add(Input(shape = (10, 5000)))
    model.add(SimpleRNN(512, return_sequences = True))
    model.add(SimpleRNN(256, return_sequences = True))
    model.add(SimpleRNN(256))
    model.add(Dense(5000, activation = 'softmax'))

    return model

model = build_model()
model.summary()

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])
history = model.fit(x_train_rnn, y_train_rnn, epochs = 1, validation_split = 0.2)